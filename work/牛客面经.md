

# 直线麦克风阵列怎么分左右

面阵，十字阵才能分

# 频率混叠 栅栏效应

如果不满足采样定理，则会发生频率折叠现象。

采样定理告诉我们，对于一个最高频率为 $f_{\text{max}}$ 的连续信号，其最高可以采样频率为 $2f_{\text{max}}$。如果采样频率小于 $2f_{\text{max}}$，则会发生频率折叠。

频率折叠的现象是：在频域中的高频部分会折叠到低频部分，造成频率冗余和信号失真。因此，我们在采样信号时必须遵循采样定理，以确保采样后的信号能够准确反映原始信号的频率特征。以上内容由chatgpt回答

栅栏效应是因为DFT计算的频谱被限制在基频的整数倍而不可能将频谱视为一个连续函数而产生的。就一定意义而言，栅栏效应表现为用DFT计算整个频谱时，就好像通过一个“栅栏”来观看一个图景一样，只能在离散点的地方看到真实图景。

增加频域抽样点数N，同时在不改变时域数据的情况下，在时域数据末端添加一些零值点，使得谱线更密，这样就可以减小栅栏效应，观察到原来看不到的频谱分量。注意，该方法通过补零来增加N，此时采样频率f(s)会随之成正比上升，又由于频率分辨率F=f(s)/N，频率分辨率不改变，也就是说，补零不改变频率分辨率。

# RNN GRU LSTM



# FIR IIR


  

滤波器可分为两种，IIR（无限冲激响应）滤波器和FIR（有限冲激响应）滤波器。

**（1）FIR和IIR滤波器的不同**

FIR滤波器的冲激响应在有限时间内衰减为0，输出仅取决于当前和过去的输入信号值，在Z域上其极点位置只能是原点，而IIR滤波器的冲激响应会无限持续，输出不仅取决于当前和过去的输入信号，还和过去的输出有关，IIR的极点可以处于单位圆内任何地方。  
设计同样参数的滤波器，FIR要比IIR需要更多的参数，也就是在处理时需要更长的时间去计算，实时性差一些。  
FIR具有线性相位，IIR不具有，非线性相位是指对于不同的频率分量造成的相位差与频率不成比例，使得输出时不同频率分量的叠加的相位值和输入时有变化，从而导致了信号的失真。因此在进行IIR设计的时候需考虑这些，如有相位要求需添加相位校准网络。  
在实际应用中，如果滤波器通带内不要求线性相位，则使用IIR，若有要求，则根据相位失真度、计算量、复杂度等因素综合考虑是选择FIR还是选择IIR+相位补偿。

**（2）FIR和IIR设计方法**

1FIR：窗函数法、频率采样法、切比雪夫逼近法。对比这三种方法，窗函数法是最早提出的，缺少关键频率的精度控制，如用该种方法设计的低通滤波器，它的截止频率依赖于窗函数的类型和滤波器长度M，并不能从截止频率出发进行设计。频率采样法指定了一些w处H（w）的值，并规定了过渡带为2π/M的倍数，由于这种H（w）在过渡带以外的其他频率上为0或1的曲线特性，这种方法用于实现频域滤波。切比雪夫逼近法在技术指标的管控上比前两种都要好，可以按照参数wp，ws，δ1，δ2，M给定的技术指标，指定参数wp，ws，δ1，M，然后利用δ2来优化滤波器，这种方法后续可着重研究一下。  
IIR：由于模拟滤波器是一个充分研究的成熟领域，多使用模拟滤波器转换到数字滤波器上。导数逼近法、冲击不变法、双线性变换法。前两种方法有严重的局限性，仅适合于低通滤波器和一类有限的带通滤波器，双线性变换法则无此限制。常用的模拟滤波器有巴特沃斯滤波器、切比雪夫滤波器、椭圆滤波器、贝塞尔滤波器。


# 重采样

[[信号的抽取与插值与子带滤波器组]]
[[语音基础/重采样]]

# MFCC

[[MFCC]]


# 频率分辨率

![image](https://cdn.staticaly.com/gh/andyye1999/image-hosting@master/20230107/image.rns6p7jmqxs.webp)

![image](https://cdn.staticaly.com/gh/andyye1999/image-hosting@master/20230107/image.1etnrunmea8.webp)

![image](https://cdn.staticaly.com/gh/andyye1999/image-hosting@master/20230107/image.3j2cww9uekm0.webp)


![image](https://cdn.staticaly.com/gh/andyye1999/image-hosting@master/20230107/image.359syy3mfym0.webp)




# 相关与卷积的区别与联系

![image](https://cdn.staticaly.com/gh/andyye1999/image-hosting@master/20230107/image.701o64broe00.webp)



# 圆周卷积和线性卷积

[[线性卷积圆周卷积]]

# 加窗 不同窗特点

对连续的语音分帧做STFT处理，等价于截断一段时间信号，对其进行周期性延拓，从而变成无限长序列，并对该无限长序列做FFT变换，这一截断并不符合傅里叶变换的定义。因此，会导致频谱泄露和混叠

频谱泄露会导致幅度较小的频点淹没在幅度较大的频点泄露分量中，

而混叠会在分段拼接处引入虚假的峰值，进而不能获得准确的频谱情况

加窗是为了抑制频谱泄露和混叠的产生

不同的窗函数有不同的特点。一般来说，选择窗函数要考虑的因素有：

1.  带噪比：窗函数的形状直接影响语音分帧的带噪比，比如矩形窗带来的噪音大于汉宁窗。
    
2.  稳定性：窗函数的形状直接影响语音分帧的稳定性，比如汉宁窗与语音信号更加稳定。
    
3.  计算复杂度：不同的窗函数具有不同的计算复杂度，例如汉宁窗与矩形窗。
    

常见的窗函数包括：矩形窗、汉宁窗、Hann窗、Hamming窗、Blackman窗等。每种窗函数的适用场景都不同，取决于应用需求。以上不同窗特点为chatGPT回答

# 推导FFT



# 损失函数的设计思路，以及常见的损失函数



# 梯度爆炸和梯度消失问题的成因和缓解

梯度爆炸的成因： 在深度学习中，梯度爆炸是指在计算损失函数的梯度时，因某些原因使得梯度值变得非常大，从而导致训练无法正常进行。通常发生在链式求导时，因为每层的梯度都是从上一层传递下来的，当上一层的梯度值很大时，就可能导致梯度爆炸。

梯度消失的成因： 梯度消失是指在计算损失函数的梯度时，因某些原因使得梯度值变得非常小，从而导致训练无法正常进行。通常发生在使用非线性激活函数（如sigmoid）的网络中，当激活函数的输入较大时，激活函数的导数很小，从而导致梯度值很小。

缓解梯度爆炸和梯度消失的方法：

-   使用更多的Batch Normalization
-   使用更大的学习率
-   使用更小的模型
-   在链式求导的过程中使用更强的正则化方法，如Dropout
-   使用更复杂的激活函数，如ReLU
-   使用更好的优化器，如Adam

以上由chatgpt回答

# 几种不同的归一化方式的区别和联系

归一化是一种常见的数据预处理方法，目的是将数据转换为固定范围内的数值。常用的几种归一化方式有以下几种：

1.  Min-Max 归一化：这种归一化方法通过将数据的最大值和最小值缩放到0-1的范围内。公式为：x' = (x-x_min)/(x_max-x_min)
    
2.  Z-Score 归一化：这种归一化方法通过计算数据的均值和标准差，将数据标准化为均值为0，标准差为1的正态分布数据。公式为：x' = (x-μ)/σ
    
3.  小数定标归一化：这种归一化方法通过计算数据的最大的整数位数，将数据的最大值归一化为1，其他数据相对地缩放。
    
4.  极差归一化：这种归一化方法通过计算数据的极差，将数据缩放到0-1的范围内。公式为：x' = (x-x_min)/(x_max-x_min)


以上四种归一化方式各有优劣，具体选择哪种归一化方式取决于数据的特征、数据处理目的以及其他因素。此外，不同归一化方式在模型训练时可能会影响模型的训练结果，因此需要根据具体场景进行选择。

以上由chatgpt回答

# dropout的原理，训练和推理的不同



# 正则化的原理以及使用场景



# 几种卷积变体：空洞卷积、深度可分离卷积




# CNN和RNN的区别和使用场景

CNN是卷积神经网络的简称，卷积神经网络一般由三个部分组成--输入层，输出层，和隐藏层。其核心的操作是卷积操作。卷积操作本质上是一种分组函数，CNN使用卷积来筛选数据并查找信息。卷积神经网络的特点是使用固定大小的输入和输出，并且学习的特征倾向于空间特征，因此卷积神经网络更适用于图像和视频的处理。RNN是循环神经网络的简称，与卷积神经网络不同的是，RNN中存在记忆单元，可以接受之前的信息指导当前的数据处理。循环神经网络的特点是可以接受变长的输出并产生变长的输出，并且学习的特征倾向于时间特征，或者说序列特征，因此循环神经网络更适用与语音和文本的处理。但是其实在具体领域这两者的应用没有具体分界，谁的性能好就用谁呗。


# 过拟合

一是扩充数据集，并提高数据多样性，可用的方法如数据增强，增加数据等；二是提前停止训练，在检测到过拟合时就提前将训练停止；三是使用正则化，包括范数正则化和Dropout等；四是削减模型的参数。

# 互相关



# FFT复杂度

![image](https://cdn.staticaly.com/gh/andyye1999/image-hosting@master/20230108/image.35gw6tlg3980.webp)

# 阵列幅度失真



幅度失真是因为时延估计和噪声协方差矩阵不准确造成的。而之所以不准确是因为，**时延估计和噪声协方差矩阵不准确**会造成**波束的主瓣方向出现偏差**，使**主瓣方向无法对准声源方向**，这样的话语音就会被抑制。


# PCM和WAV文件中存储的是啥

[[PCM WAV]]

# 采样率和采样深度的物理意义


采样位深，音频的位深度决定动态范围。我们常见的16Bit（16比特），可以记录大概96分贝的动态范围。那么，您可以大概知道，每一个比特大约可以记录6分贝的声音。同理，20Bit可记录的动态范围大概就是120dB；24Bit就大概是144dB。音频位速，也叫码率，或者比特率。位速是指在一个数据流中每秒钟能通过的信息量，也可以理解为：每秒钟用多少比特的数据量去表示。96kbps的WMA音频格式的音质明显要比96kbps的MP3音质好。为什么会这样呢？因为不同的压缩算法，对数据的利用率不同而造成的差异。再举例，假如MP3压缩至48kbps以下，已经惨不忍睹，而如果是AAC音频格式，同样是48kbps的位速下，音质明显比MP3好。


# 128点FFT变换，16kHz的音频，可以分为多少个子带？



# 维纳滤波和谱减法的区别



# 时延估计的实现



# WebRTC NS中的噪声估计用到的三个特征值是什么？它们分别的定义是什么？

[[webrtc_ns#FeatureUpdate() 提取平均LRT参数、频谱差异、频谱平坦度]]

# 你用的VAD的是做什么用的，原理是什么，遇到突发噪声怎么处理？



# 麦克风阵列的物理结构特性有哪些指标？你的麦克风阵列的指标都是怎样的？


麦克风阵列的物理结构特性的指标有以下几点：

1.  指向性：指麦克风阵列能够捕捉到的声音的方向性。一般情况下，麦克风阵列的指向性与麦克风的布局有关。
    
2.  分辨率：指麦克风阵列能够区分声音的位置的精度。麦克风阵列的分辨率取决于麦克风数量和布局。
    
3.  灵敏度：指麦克风阵列能够捕捉到声音的最小强度。灵敏度越高，麦克风阵列能够捕捉到的声音越弱。
    
4.  噪声级：指麦克风阵列自身产生的噪声。噪声级越低，麦克风阵列的性能越好。
    
5.  动态范围：指麦克风阵列能够捕捉到的声音的最大和最小强度差。动态范围越大，麦克风阵列的性能越好。
    
6.  过滤效果：指麦克风阵列能够滤掉外界杂音的效果。过滤效果越好，麦克风阵列的性能越好。
    
7.  抗干扰能力：指麦克风阵列在干扰环境下的工作效果。抗干扰能力越强，麦克风阵列的性能越好。

以上是由chatgpt回答


# 都用了哪些噪声种类？现在机器学习几百种噪声，为什么用这么少？



# 在哪些噪声环境下语音增强的效果比较差？



# 增强语音的结果的衡量标准是什么？


语音增强结果的衡量标准主要包括：

1.  听觉评价：根据人耳的听觉感受来评价语音的质量。例如 Mean Opinion Score (MOS)。
    
2.  信噪比：语音信号与噪声的比值。
    
3.  声音干净度：语音信号中的噪声减少了多少。
    
4.  语音信息内容保留度：增强语音信号与原始语音信号的相似度。
    
5.  语音特征保留度：语音增强后语音特征的保留情况，例如语音语调、语速、音高、音色等。

不同的语音增强任务可能需要不同的衡量标准，对于每一种语音增强算法，其衡量标准通常要结合其具体任务和应用场景进行评估。
以上由chatgpt进行回答






