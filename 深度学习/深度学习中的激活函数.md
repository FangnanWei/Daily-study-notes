# [深度学习中的激活函数](https://www.cnblogs.com/LXP-Never/p/9771869.html)  
众所周知神经网络单元是由线性单元和非线性单元组成的，一般神经网络的计算时线性的，而非线性单元就是我们今天要介绍的--激活函数，不同的激活函数得出的结果也是不同的。他们也各有各的优缺点，虽然激活函数有自己的发展历史，不断的优化，但是如何在众多激活函数中做出选择依然要看我们所实现深度学习实验的效果。

![](https://img2018.cnblogs.com/blog/1433301/201907/1433301-20190721230409923-204274869.png)

　　这篇博客会介绍一些常用的激活函数：Sigmoid、tanh、ReLU、LeakyReLU、maxout。以及一些较为冷门的激活函数：PRelu、ELU、SELU

# sigmoid

sigmoid激活函数将输入(−∞,+∞)映射到(0,1)之间，他的数学函数为：

σ(z)=11+e−z

![](https://images.cnblogs.com/OutliningIndicators/ContractedBlock.gif) View Code

![](https://img2020.cnblogs.com/blog/1433301/202006/1433301-20200624094204509-576467717.png)![](https://img2020.cnblogs.com/blog/1433301/202006/1433301-20200624094212108-1538824319.png)

　　历史上sigmoid非常常用，但是由于他的两个缺点，实际很少用了，现在看到sigmoid激活函数，都是在新手教程中做一些简单的实验。

### 优点

1.  它能够把输入的连续实值变换为0和1之间的输出，适合做概率值的处理。
    -   如果是非常大的负数，那么输出就是0
    -   如果是非常大的正数，输出就是1

### 缺点

**1、梯度消失**

　　我们从上图可以看出，当x较大或者较小时，sigmoid输出趋近0或1，导数接近0，而后向传递的数学依据是微积分求导的**链式法则**，当前层的导数需要之前各层导数的乘积，几个小数的相乘，结果会很接近0。Sigmoid导数的最大值是0.25，这意味着导数在每一层至少会被压缩为原来的1/4，通过两层后被变为1/16，…，通过10层后为1/1048576。这种情况就是梯度消失。梯度一旦消失，参数不能沿着loss降低的方向优化，

**2、不是以零为中心**

　　通过Sigmoid函数我们可以知道，Sigmoid的输出值恒大于0，输出不是0均值（既zero-centerde），这会导致后一层的神经元将得到上一层输出的非均值的输入。

　　举例来讲σ(∑iwixi+b)，如果xi恒大于0，那么对其wi的导数总是正数或总是负数，向传播的过程中w要么都往正方向更新，要么都往负方向更新，导致有一种捆绑的效果，使得收敛缓慢。且可能导致陷入局部最小值。当然了，**如果按batch去训练，那么那个batch可能得到不同的信号，所以这个问题还是可以缓解一下的**。

![](https://img2018.cnblogs.com/blog/1433301/201910/1433301-20191015090930036-646721163.png)

**3、运算量大：**

　　解析式中含有幂运算，计算机求解时相对来讲比较耗时。对于规模比较大的深度网络，这会较大地增加训练时间。
  
  # tanh

　　Tanh 激活函数又叫作双曲正切激活函数（hyperbolic tangent activation function）。与 Sigmoid 函数类似，但 Tanh 函数将其压缩至-1 到 1 的区间内，输出是zero-centered的（零为中心），在实践中，Tanh 函数的使用优先性高于 Sigmoid 函数。负数输入被当作负值，零输入值的映射接近零，正数输入被当作正值。

数学函数为：

f(z)=tanh(z)=ez−e−zez+e−z

![](https://images.cnblogs.com/OutliningIndicators/ContractedBlock.gif) View Code

![](https://img2020.cnblogs.com/blog/1433301/202006/1433301-20200624100329439-1302804260.jpg)![](https://img2020.cnblogs.com/blog/1433301/202006/1433301-20200624100344924-1930028669.png)

### 优点

1.  sigmoid的优点他都有，另外 tanh的输出是zero-centered，以0为中心

### 缺点

1、**特殊情况存在梯度消失问题**

　　当输入值过大或者过小，提取趋近于0，失去敏感性，处于饱和状态。

# ReLU

这才是一个目前主流论文中非常常用的激活函数，它的数学公式为：

f(x)=max(0,x)

![](https://images.cnblogs.com/OutliningIndicators/ContractedBlock.gif) View Code

![](https://img2020.cnblogs.com/blog/1433301/202006/1433301-20200624102136063-946191443.png)![](https://img2020.cnblogs.com/blog/1433301/202006/1433301-20200624102153458-1756116431.png)

### 优点

1.  ReLU的计算量小，收敛速度很快，因为sigmoid和tanh，ReLU有指数运算
2.  在**正区间(x>0)**解决了梯度消失问题。图像数据是在(0~255)之间，即便归一化处理值也大于0，但是音频数据有正有负，不适合relu函数

### 缺点：

1.  ReLU的输出不是zero-centered
2. ** RuLU在训练的时候很容易导致神经元“死掉”**

**死掉：一个非常大的梯度经过一个 ReLU 神经元，更新过参数之后，这个神经元再也不会被任何数据激活相应的权重永远不会更新。有两种原因导致这种情况：1、非常不幸的初始化。2、学习率设置的太高导致在训练过程中参数更新太大，解决方法是使用Xavier初始化方法，合理设置学习率，会降低这种情况的发生概率。或使用Adam等自动调节学习率的算法。**

补充：ReLU相比sigmoid和tanh的一个缺点是没有对上界设限，在实际使用中,可以设置一个上限,**如ReLU6经验函数: f(x)=min(6,max(0,x))**
# SoftMax

softmax用于多分类神经网络输出，如果某一个ai打过其他z，那这个映射的分量就逼近1，其他就逼近0，主要应用于“**分类**”。

，SOFTMAX:ai=σi(z)=ezi∑j=1mezj，zi=wix+b

作用：把神经元中线性部分输出的得分值(score)，转换为概率值。softmax输出的是(归一化)概率，

　　含有softmax激活函数的网络层有这样一个性质：∑i=1jσi(z)=1，可以解释为每个节点的输出值小于等于1。**softmax激励函数通常在神经网络的最后一层作为**分类器**的输出，输出值(概率)最大的即为分类结果。**

猫狗猫:(0.050.050.70.2)狗:(0.80.060.010.04)

一般用softmax激活函数的都会用**交叉熵损失函数**，顺带讲一下

假如从盒中黑球的个数为16，白球的个数为14，那么从盒中抽到黑球的**先验概率**为1630，熵越大，事件的不确定性越大。

　　交叉熵的表达式为：L=−∑iyilog⁡ai

　　式中ai是softmax激励函数的输出，yi是线性单元的输出值。

　　交叉熵所描述的是经过**训练分类结果**的信息熵和**测试集分类结果**的信息熵之间的**差距**。在拟合过程中产生的熵是有差距的，这个差距由交叉熵来定义。那么只要熵差越小，就越接近真实值。

$$标签分类结果：(1000)

训练分类结果：(0.80.050.10.05)$$

使用交叉熵作为损失函数可以避免梯度消散。